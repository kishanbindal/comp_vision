{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041b62ba",
   "metadata": {},
   "source": [
    "# Object Tracking\n",
    "\n",
    "Object tracking is a computer vision problem that invovles `Detection` of objects(features) and `Tracking`.\n",
    "\n",
    "> `Detection` referes to the task of observing a scene and finding points of interest within the scene. \n",
    "\n",
    "> `Tracking` refers to the task of predicting future positions of a `detected object`, using it's previous known `states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe351844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from deep_sort.tracker import Tracker\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eca699ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the file along with filepaths\n",
    "import os\n",
    "\n",
    "filename=\"traffic_unedited.mp4\"\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "video_path = os.path.join(current_dir, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e5aac",
   "metadata": {},
   "source": [
    "## Deep in DeepSORT\n",
    "\n",
    "In DeepSort, a pre-trained deep learning model is responsible for feature extraction as well as returning the bounding boxes that enclose the features. \n",
    "\n",
    "In our implementation, since the bounding box is labeled by the User for the first frame, we will not be using an object detection algorithm. Instead we will `MobileNetv2` as a `Feature Extractor`\n",
    "\n",
    "This will help the Kalman Filter in the SORT algorithm to update it's weights more effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b639ae",
   "metadata": {},
   "source": [
    "Input shape into the Feature Detector will play a huge role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac3d1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping pretrained model ready to derive feature vector for objects within the bounding boxes\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "class FeatureDetector():\n",
    "    \n",
    "    detector = None\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_detector(self):\n",
    "        self.detector = MobileNetV2(input_shape=(128, 128, 3), weights=\"imagenet\")\n",
    "        print(self.detector)\n",
    "\n",
    "    # Get Feature vector from model.\n",
    "    def predict(self, cropped_image):\n",
    "        return self.detector.predict(cropped_image)\n",
    "\n",
    "    def _standardize_input(self, cropped_img):\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            tf.keras.layers.Rescaling(1./255)\n",
    "        ])\n",
    "        rescaled_img = data_augmentation(cropped_img)\n",
    "        return rescaled_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c2a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_from_bbox(frame, bbox):\n",
    "    \"\"\"\n",
    "    Takes in the Frame and bounding box.\n",
    "    Returns the Cropped Image with bbox height and width.\n",
    "    \"\"\"\n",
    "    x_start = bbox[0]\n",
    "    y_start = bbox[1]\n",
    "    width = bbox[2]\n",
    "    height = bbox[3]\n",
    "    print(f\"end x: {x_start+ width}\\nend y: {y_start+height}\")\n",
    "    cropped_img = frame[ y_start:y_start+height,x_start:x_start+width]\n",
    "    print(cropped_img.shape)\n",
    "    return cropped_img\n",
    "\n",
    "def resize_to_model_input(cropped_image):\n",
    "    resized_image = cv2.resize(cropped_image, dsize=(128, 128))\n",
    "    return resized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8a7b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "# Draw the bounding box in the first frame\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(total_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de9e50e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389, 34, 73, 66)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret, frame = cap.read()\n",
    "cv2.imshow(\"Annotation\", frame)\n",
    "\n",
    "bbox = cv2.selectROI(\"Annotation\", frame, fromCenter=False, showCrosshair=False)\n",
    "##bbox returned in the format\n",
    "# x_start, y_start, width, height\n",
    "cv2.destroyAllWindows()\n",
    "# bbox = [bbox[0]]\n",
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdfc4f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end x: 462\n",
      "end y: 100\n",
      "(66, 73, 3)\n"
     ]
    }
   ],
   "source": [
    "cropped_image = crop_from_bbox(frame, bbox)\n",
    "# cv2.imshow(\"Cropped Image\", cropped_image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b61c7341",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_img = resize_to_model_input(cropped_image)\n",
    "# cv2.imshow(\"Resized Image\", resized_img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e86e7",
   "metadata": {},
   "source": [
    "Now that we have rescaled and resized our image for the model, our `helper functions` are working as expected. \n",
    "We are ready to extract a feature vector given the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75d9125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.nn_matching import NearestNeighborDistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40285496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialise the tracker object\n",
    "metric = NearestNeighborDistanceMetric(\"cosine\", 0.5)\n",
    "tracker = Tracker(metric=metric, n_init=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79a0057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = list(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01aa59be",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_detection = [[[bbox[0], bbox[1], bbox[2], bbox[3]], 1.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a23258a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 128, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(resized_img, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ec90f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.detection import Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7482309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\cv\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:562: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "<keras.engine.functional.Functional object at 0x000001F85B5A9840>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\cv\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>]\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    feature_detector = FeatureDetector()\n",
    "    feature_detector.create_detector()\n",
    "    feature_vec = feature_detector.predict(np.expand_dims(resized_img, axis=0))\n",
    "    raw_detections = [Detection(bbox, 1.0, feature_vec)]\n",
    "    subsequent_bbox = []\n",
    "    # tracker.predict()\n",
    "    tracker.update(raw_detections)\n",
    "    # for frame_index in range(total_frames)[1:5]:\n",
    "    #     cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "    #     ret, frame = cap.read()\n",
    "    #     if ret:\n",
    "    #         bb = cv2.selectROI(\"Annotate frame\", frame, fromCenter=False, showCrosshair=False)\n",
    "    #         cv2.destroyAllWindows()\n",
    "    #         subsequent_bbox.append(bb)\n",
    "    # for bb in subsequent_bbox:\n",
    "    #     print(\"here\")\n",
    "    #     raw_detections = [Detection(bb, 1.0, feature_vec)]\n",
    "        # tracker.predict()\n",
    "        # tracker.update(raw_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1259c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KMP_DUPLICATE_LIB_OK=TRUE\n"
     ]
    }
   ],
   "source": [
    "%env KMP_DUPLICATE_LIB_OK=TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98c1c38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[388  34 462 100]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>]\n",
      "1\n",
      "[389  33 461 104]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>]\n",
      "1\n",
      "[388  34 462 100]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>, <deep_sort.detection.Detection object at 0x000001F85A1A7730>]\n",
      "1\n",
      "[389  33 460 104]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>, <deep_sort.detection.Detection object at 0x000001F85A1A7730>, <deep_sort.detection.Detection object at 0x000001F85A1A7C70>]\n",
      "1\n",
      "[392  37 460 100]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>, <deep_sort.detection.Detection object at 0x000001F85A1A7730>, <deep_sort.detection.Detection object at 0x000001F85A1A7C70>, <deep_sort.detection.Detection object at 0x000001F858D925C0>]\n",
      "1\n",
      "[386  36 463 100]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>, <deep_sort.detection.Detection object at 0x000001F85A1A7730>, <deep_sort.detection.Detection object at 0x000001F85A1A7C70>, <deep_sort.detection.Detection object at 0x000001F858D925C0>, <deep_sort.detection.Detection object at 0x000001F85A1D61A0>]\n",
      "1\n",
      "[388  34 462 100]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>, <deep_sort.detection.Detection object at 0x000001F85A1A7730>, <deep_sort.detection.Detection object at 0x000001F85A1A7C70>, <deep_sort.detection.Detection object at 0x000001F858D925C0>, <deep_sort.detection.Detection object at 0x000001F85A1D61A0>, <deep_sort.detection.Detection object at 0x000001F85A1D4400>]\n",
      "1\n",
      "[389  33 460 104]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>, <deep_sort.detection.Detection object at 0x000001F85A1A7730>, <deep_sort.detection.Detection object at 0x000001F85A1A7C70>, <deep_sort.detection.Detection object at 0x000001F858D925C0>, <deep_sort.detection.Detection object at 0x000001F85A1D61A0>, <deep_sort.detection.Detection object at 0x000001F85A1D4400>, <deep_sort.detection.Detection object at 0x000001F85A22F880>]\n",
      "1\n",
      "[392  37 460 100]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>, <deep_sort.detection.Detection object at 0x000001F85A1A7730>, <deep_sort.detection.Detection object at 0x000001F85A1A7C70>, <deep_sort.detection.Detection object at 0x000001F858D925C0>, <deep_sort.detection.Detection object at 0x000001F85A1D61A0>, <deep_sort.detection.Detection object at 0x000001F85A1D4400>, <deep_sort.detection.Detection object at 0x000001F85A22F880>, <deep_sort.detection.Detection object at 0x000001F85A1D6440>]\n",
      "1\n",
      "[386  36 463 100]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>, <deep_sort.detection.Detection object at 0x000001F85A1A7730>, <deep_sort.detection.Detection object at 0x000001F85A1A7C70>, <deep_sort.detection.Detection object at 0x000001F858D925C0>, <deep_sort.detection.Detection object at 0x000001F85A1D61A0>, <deep_sort.detection.Detection object at 0x000001F85A1D4400>, <deep_sort.detection.Detection object at 0x000001F85A22F880>, <deep_sort.detection.Detection object at 0x000001F85A1D6440>, <deep_sort.detection.Detection object at 0x000001F85B499BD0>]\n",
      "1\n",
      "[384  36 466  99]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>, <deep_sort.detection.Detection object at 0x000001F85A1A7730>, <deep_sort.detection.Detection object at 0x000001F85A1A7C70>, <deep_sort.detection.Detection object at 0x000001F858D925C0>, <deep_sort.detection.Detection object at 0x000001F85A1D61A0>, <deep_sort.detection.Detection object at 0x000001F85A1D4400>, <deep_sort.detection.Detection object at 0x000001F85A22F880>, <deep_sort.detection.Detection object at 0x000001F85A1D6440>, <deep_sort.detection.Detection object at 0x000001F85B499BD0>, <deep_sort.detection.Detection object at 0x000001F85A22DA80>]\n",
      "1\n",
      "[383  33 470 106]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>, <deep_sort.detection.Detection object at 0x000001F85A1A7730>, <deep_sort.detection.Detection object at 0x000001F85A1A7C70>, <deep_sort.detection.Detection object at 0x000001F858D925C0>, <deep_sort.detection.Detection object at 0x000001F85A1D61A0>, <deep_sort.detection.Detection object at 0x000001F85A1D4400>, <deep_sort.detection.Detection object at 0x000001F85A22F880>, <deep_sort.detection.Detection object at 0x000001F85A1D6440>, <deep_sort.detection.Detection object at 0x000001F85B499BD0>, <deep_sort.detection.Detection object at 0x000001F85A22DA80>, <deep_sort.detection.Detection object at 0x000001F85B49BEB0>]\n",
      "1\n",
      "[389  34 467 106]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>, <deep_sort.detection.Detection object at 0x000001F85A1A7730>, <deep_sort.detection.Detection object at 0x000001F85A1A7C70>, <deep_sort.detection.Detection object at 0x000001F858D925C0>, <deep_sort.detection.Detection object at 0x000001F85A1D61A0>, <deep_sort.detection.Detection object at 0x000001F85A1D4400>, <deep_sort.detection.Detection object at 0x000001F85A22F880>, <deep_sort.detection.Detection object at 0x000001F85A1D6440>, <deep_sort.detection.Detection object at 0x000001F85B499BD0>, <deep_sort.detection.Detection object at 0x000001F85A22DA80>, <deep_sort.detection.Detection object at 0x000001F85B49BEB0>, <deep_sort.detection.Detection object at 0x000001F85A4475E0>]\n",
      "1\n",
      "[387  33 463 100]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>, <deep_sort.detection.Detection object at 0x000001F85A1A7730>, <deep_sort.detection.Detection object at 0x000001F85A1A7C70>, <deep_sort.detection.Detection object at 0x000001F858D925C0>, <deep_sort.detection.Detection object at 0x000001F85A1D61A0>, <deep_sort.detection.Detection object at 0x000001F85A1D4400>, <deep_sort.detection.Detection object at 0x000001F85A22F880>, <deep_sort.detection.Detection object at 0x000001F85A1D6440>, <deep_sort.detection.Detection object at 0x000001F85B499BD0>, <deep_sort.detection.Detection object at 0x000001F85A22DA80>, <deep_sort.detection.Detection object at 0x000001F85B49BEB0>, <deep_sort.detection.Detection object at 0x000001F85A4475E0>, <deep_sort.detection.Detection object at 0x000001F858C2DAE0>]\n",
      "1\n",
      "[388  34 462 100]\n",
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>, <deep_sort.detection.Detection object at 0x000001F85A1A7730>, <deep_sort.detection.Detection object at 0x000001F85A1A7C70>, <deep_sort.detection.Detection object at 0x000001F858D925C0>, <deep_sort.detection.Detection object at 0x000001F85A1D61A0>, <deep_sort.detection.Detection object at 0x000001F85A1D4400>, <deep_sort.detection.Detection object at 0x000001F85A22F880>, <deep_sort.detection.Detection object at 0x000001F85A1D6440>, <deep_sort.detection.Detection object at 0x000001F85B499BD0>, <deep_sort.detection.Detection object at 0x000001F85A22DA80>, <deep_sort.detection.Detection object at 0x000001F85B49BEB0>, <deep_sort.detection.Detection object at 0x000001F85A4475E0>, <deep_sort.detection.Detection object at 0x000001F858C2DAE0>, <deep_sort.detection.Detection object at 0x000001F858C2C2E0>]\n",
      "1\n",
      "[389  33 460 104]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CV\\Object tracking and Image Segmentation\\deep_sort\\deep_sort\\detection.py:48: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret[2] /= ret[3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det [<deep_sort.detection.Detection object at 0x000001F858AF66E0>, <deep_sort.detection.Detection object at 0x000001F85A203220>, <deep_sort.detection.Detection object at 0x000001F85A203280>, <deep_sort.detection.Detection object at 0x000001F85A1A7730>, <deep_sort.detection.Detection object at 0x000001F85A1A7C70>, <deep_sort.detection.Detection object at 0x000001F858D925C0>, <deep_sort.detection.Detection object at 0x000001F85A1D61A0>, <deep_sort.detection.Detection object at 0x000001F85A1D4400>, <deep_sort.detection.Detection object at 0x000001F85A22F880>, <deep_sort.detection.Detection object at 0x000001F85A1D6440>, <deep_sort.detection.Detection object at 0x000001F85B499BD0>, <deep_sort.detection.Detection object at 0x000001F85A22DA80>, <deep_sort.detection.Detection object at 0x000001F85B49BEB0>, <deep_sort.detection.Detection object at 0x000001F85A4475E0>, <deep_sort.detection.Detection object at 0x000001F858C2DAE0>, <deep_sort.detection.Detection object at 0x000001F858C2C2E0>, <deep_sort.detection.Detection object at 0x000001F858C970A0>]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matrix contains invalid numeric entries",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     15\u001b[0m     raw_detections\u001b[39m.\u001b[39mappend(Detection(bb, \u001b[39m1.0\u001b[39m, feature_vec))\n\u001b[1;32m---> 16\u001b[0m     tracker\u001b[39m.\u001b[39;49mupdate(raw_detections)\n\u001b[0;32m     17\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     18\u001b[0m cv2\u001b[39m.\u001b[39mrectangle(frame,\n\u001b[0;32m     19\u001b[0m               (bbox_updated[\u001b[39m0\u001b[39m], bbox_updated[\u001b[39m1\u001b[39m]),\n\u001b[0;32m     20\u001b[0m               (bbox_updated[\u001b[39m2\u001b[39m], bbox_updated[\u001b[39m3\u001b[39m]),\n\u001b[0;32m     21\u001b[0m               (\u001b[39m255\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[0;32m     22\u001b[0m             )\n",
      "File \u001b[1;32md:\\CV\\Object tracking and Image Segmentation\\deep_sort\\deep_sort\\tracker.py:70\u001b[0m, in \u001b[0;36mTracker.update\u001b[1;34m(self, detections)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39m# Run matching cascade.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdet\u001b[39m\u001b[39m\"\u001b[39m, detections)\n\u001b[0;32m     69\u001b[0m matches, unmatched_tracks, unmatched_detections \u001b[39m=\u001b[39m \\\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_match(detections)\n\u001b[0;32m     71\u001b[0m \u001b[39m# Update track set.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m track_idx, detection_idx \u001b[39min\u001b[39;00m matches:\n",
      "File \u001b[1;32md:\\CV\\Object tracking and Image Segmentation\\deep_sort\\deep_sort\\tracker.py:125\u001b[0m, in \u001b[0;36mTracker._match\u001b[1;34m(self, detections)\u001b[0m\n\u001b[0;32m    118\u001b[0m iou_track_candidates \u001b[39m=\u001b[39m unconfirmed_tracks \u001b[39m+\u001b[39m [\n\u001b[0;32m    119\u001b[0m     k \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m unmatched_tracks_a \u001b[39mif\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracks[k]\u001b[39m.\u001b[39mtime_since_update \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m    121\u001b[0m unmatched_tracks_a \u001b[39m=\u001b[39m [\n\u001b[0;32m    122\u001b[0m     k \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m unmatched_tracks_a \u001b[39mif\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracks[k]\u001b[39m.\u001b[39mtime_since_update \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m    124\u001b[0m matches_b, unmatched_tracks_b, unmatched_detections \u001b[39m=\u001b[39m \\\n\u001b[1;32m--> 125\u001b[0m     linear_assignment\u001b[39m.\u001b[39;49mmin_cost_matching(\n\u001b[0;32m    126\u001b[0m         iou_matching\u001b[39m.\u001b[39;49miou_cost, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iou_distance, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtracks,\n\u001b[0;32m    127\u001b[0m         detections, iou_track_candidates, unmatched_detections)\n\u001b[0;32m    129\u001b[0m matches \u001b[39m=\u001b[39m matches_a \u001b[39m+\u001b[39m matches_b\n\u001b[0;32m    130\u001b[0m unmatched_tracks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(unmatched_tracks_a \u001b[39m+\u001b[39m unmatched_tracks_b))\n",
      "File \u001b[1;32md:\\CV\\Object tracking and Image Segmentation\\deep_sort\\deep_sort\\linear_assignment.py:58\u001b[0m, in \u001b[0;36mmin_cost_matching\u001b[1;34m(distance_metric, max_distance, tracks, detections, track_indices, detection_indices)\u001b[0m\n\u001b[0;32m     55\u001b[0m cost_matrix \u001b[39m=\u001b[39m distance_metric(\n\u001b[0;32m     56\u001b[0m     tracks, detections, track_indices, detection_indices)\n\u001b[0;32m     57\u001b[0m cost_matrix[cost_matrix \u001b[39m>\u001b[39m max_distance] \u001b[39m=\u001b[39m max_distance \u001b[39m+\u001b[39m \u001b[39m1e-5\u001b[39m\n\u001b[1;32m---> 58\u001b[0m indices \u001b[39m=\u001b[39m linear_assignment(cost_matrix)\n\u001b[0;32m     59\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack([indices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mreshape(((indices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), \u001b[39m1\u001b[39m)),indices[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mreshape(((indices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), \u001b[39m1\u001b[39m))])\n\u001b[0;32m     60\u001b[0m matches, unmatched_tracks, unmatched_detections \u001b[39m=\u001b[39m [], [], []\n",
      "\u001b[1;31mValueError\u001b[0m: matrix contains invalid numeric entries"
     ]
    }
   ],
   "source": [
    "# Add the detection to the tracker\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    tracker.predict()\n",
    "    for track in tracker.tracks:\n",
    "        bbox_updated = track.to_tlbr().astype(int)\n",
    "        print(track.state)\n",
    "        print(bbox_updated)\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        cv2.rectangle(frame,\n",
    "                      (bbox_updated[0], bbox_updated[1]),\n",
    "                      (bbox_updated[2], bbox_updated[3]),\n",
    "                      (255, 0, 0)\n",
    "                    )\n",
    "        # cv2.putText(frame,\n",
    "        #             str(track.track_id),\n",
    "\n",
    "        #             \n",
    "    cv2.imshow(\"Tracking\", frame)\n",
    "    key = cv2.waitKey(30)\n",
    "    if key == ord('q') or key==27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc610a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
